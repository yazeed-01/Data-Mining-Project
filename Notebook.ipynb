{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project<br>\n",
    "## Team name: Only God Knows<br>\n",
    "Team members: <br>\n",
    "    - Alameen Sabbah<br>\n",
    "    - Yazeed Migdadi<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Take a Look at Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types and Categories\n",
    "\n",
    "In this step, we'll examine the data types of each column in the dataset and categorize them accordingly. Understanding the nature of each feature will help us decide how to handle them during the data analysis process.\n",
    "\n",
    "### Feature Description and Data Types:\n",
    "\n",
    "1. **Gender**: \n",
    "   - Values: `0: Male`, `1: Female`\n",
    "   - Data Type: **Categorical (Binary)**\n",
    "   - Description: Indicates the gender of the customer.\n",
    "\n",
    "2. **Age**:\n",
    "   - Data Type: **Numerical (Continuous)**\n",
    "   - Description: Represents the age of the customer in years.\n",
    "\n",
    "3. **Driving_License**:\n",
    "   - Values: `0: No License`, `1: Has License`\n",
    "   - Data Type: **Categorical (Binary)**\n",
    "   - Description: Indicates whether the customer has a driving license.\n",
    "\n",
    "4. **Region_Code**:\n",
    "   - Data Type: **Categorical (Ordinal)**\n",
    "   - Description: Encoded regions of the customer.\n",
    "\n",
    "5. **Previously_Insured**:\n",
    "   - Values: `0: No`, `1: Yes`\n",
    "   - Data Type: **Categorical (Binary)**\n",
    "   - Description: Indicates whether the customer has had insurance before.\n",
    "\n",
    "6. **Vehicle_Age**:\n",
    "   - Values: `1: 1-2 years`, `2: < 1 year`, `3: > 2 years`\n",
    "   - Data Type: **Categorical (Ordinal)**\n",
    "   - Description: Represents the age of the vehicle in different categories.\n",
    "\n",
    "7. **Vehicle_Damage**:\n",
    "   - Values: `0: No`, `1: Yes`\n",
    "   - Data Type: **Categorical (Binary)**\n",
    "   - Description: Indicates whether the customer's vehicle has previously been damaged.\n",
    "\n",
    "8. **Annual_Premium**:\n",
    "   - Data Type: **Numerical (Continuous)**\n",
    "   - Description: The amount of money paid annually for the insurance policy.\n",
    "\n",
    "9. **Policy_Sales_Channel**:\n",
    "   - Data Type: **Categorical (Nominal)**\n",
    "   - Description: Indicates the sales agency that dealt with the customer, identifying which agency offered the insurance service.\n",
    "\n",
    "10. **Vintage**:\n",
    "    - Data Type: **Numerical (Continuous)**\n",
    "    - Description: Represents the number of days the customer has been insured with the company.\n",
    "\n",
    "11. **Response**:\n",
    "    - Values: `0: No`, `1: Yes`\n",
    "    - Data Type: **Categorical (Binary)**\n",
    "    - Description: The target variable, indicating whether the customer responded positively to the insurance offer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Data Quality\n",
    "\n",
    "In this step, we will assess the quality of the dataset. This includes identifying:\n",
    "\n",
    "1. **Outliers**: Extreme values that may distort the analysis.\n",
    "2. **Wrong Data**: Inconsistent or invalid data entries.\n",
    "3. **Duplicate Data**: Duplicate rows that could bias the results.\n",
    "4. **Missing Data**: Columns or rows with missing values.\n",
    "5. **Class Balance**: Assessing the balance between different classes in categorical data, especially in the target variable `Response`.\n",
    "\n",
    "We will use various techniques and visualizations to identify these issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# 1. Checking for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# 2. Checking for missing data\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"\\nMissing data per column:\")\n",
    "print(missing_data)\n",
    "\n",
    "# 3. Checking for outliers in specific columns\n",
    "columns_to_check = ['Age', 'Region_Code', 'Annual_Premium', 'Vintage', 'Policy_Sales_Channel']\n",
    "\n",
    "# Ensure the columns exist in the DataFrame\n",
    "columns_to_check = [col for col in columns_to_check if col in df.columns]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "num_cols = 3  # Set number of columns for subplots\n",
    "num_rows = (len(columns_to_check) // num_cols) + (len(columns_to_check) % num_cols > 0)\n",
    "\n",
    "for i, col in enumerate(columns_to_check, 1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Checking for out-of-bound values\n",
    "# Example for Age: Assuming a reasonable range (18 to 100)\n",
    "age_outliers = df[(df['Age'] < 18) | (df['Age'] > 100)]\n",
    "print(\"\\nWrong Data (Out-of-bound Age):\")\n",
    "print(age_outliers)\n",
    "\n",
    "# Example for Annual_Premium: Defining an arbitrary range for demonstration\n",
    "premium_outliers = df[df['Annual_Premium'] > 100000]\n",
    "print(\"\\nOutliers in Annual_Premium (Above 100,000):\")\n",
    "print(premium_outliers)\n",
    "\n",
    "# 5. Checking class balance for the target variable 'Response'\n",
    "sns.countplot(x='Response', data=df)\n",
    "plt.title('Class Distribution of Response')\n",
    "plt.show()\n",
    "\n",
    "# 6. Check balance in categorical columns (if needed)\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(f'Class Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Steps\n",
    "\n",
    "The following sections outline the key preprocessing steps applied to the dataset:\n",
    "\n",
    "## 1. Attribute Transformation\n",
    "- **Log Transformation**: \n",
    "  - The `Annual_Premium` column is transformed using the natural logarithm (`log1p`). This transformation helps in reducing skewness and stabilizing the variance in the data.\n",
    "  \n",
    "- **Square Root Transformation**: \n",
    "  - The `Vintage` column is transformed using the square root (`sqrt`). This helps to reduce the impact of large values and bring the data closer to a normal distribution.\n",
    "\n",
    "## 2. Feature Engineering\n",
    "- **Creating Interaction Terms**:\n",
    "  - Interaction features are created by combining the `Age_Group` and `Vehicle_Age` categories to capture any potential combined effect these features might have on the target variable.\n",
    "\n",
    "- **Age Group Categorization**:\n",
    "  - The `Age` feature is categorized into four age groups: `Young`, `Adult`, `Middle_Aged`, and `Senior` using the `pd.cut()` function. This helps capture non-linear relationships and makes the data more interpretable.\n",
    "\n",
    "- **Additional Features**:\n",
    "  - **Age_Squared**: The square of the `Age` feature to capture quadratic effects of age.\n",
    "  - **Premium_to_Age_Ratio**: The ratio of `Annual_Premium_Log` to `Age` to capture how the premium relates to age.\n",
    "  - **Retention_Likelihood**: A binary feature indicating whether the `Vintage` (duration of the policy) is above the median, assuming this may correlate with customer retention likelihood.\n",
    "  - **Age_VehicleDamage_Interaction**: Interaction between `Age` and `Vehicle_Damage`, as the relationship between age and vehicle damage might be significant.\n",
    "\n",
    "- **Region-Based Features**:\n",
    "  - A new `Region` feature is created based on the `Policy_Sales_Channel`, where the `Policy_Sales_Channel` value is used to assign either 'North' or 'South' to the region. This can capture geographical differences in sales channels.\n",
    "\n",
    "## 3. Feature Selection\n",
    "- **Mutual Information for Feature Selection**:\n",
    "  - Feature selection is performed using the **mutual information** between the features (`X`) and the target variable (`y`). Mutual information measures the amount of information gained about one variable through another.\n",
    "  - The `mutual_info_classif` function from `sklearn.feature_selection` is used to calculate the mutual information scores between each feature and the target variable.\n",
    "  - The top `n_features` features are selected based on their mutual information scores. By default, the top 30 features are selected, but this number can be adjusted as needed.\n",
    "  \n",
    "## 4. Remove Outliers\n",
    "- **Outlier Removal Based on Standard Deviation**:\n",
    "  - Outliers are detected and removed based on the **standard deviation** from the mean of each column.\n",
    "  - The `remove_outliers` function filters out rows where the values of a given column lie outside the range of `mean ± n_std * std`, where `n_std` is a multiplier that defines how many standard deviations away from the mean the values must be in order to be considered outliers.\n",
    "  - By default, `n_std` is set to 3, meaning that any values outside of 3 standard deviations from the mean will be removed.\n",
    "  - The code for removing outliers is as follows:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, is_train=True):\n",
    "    \"\"\"Enhanced feature engineering and preprocessing.\"\"\"\n",
    "    \n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "    \n",
    "    # Apply transformations and scaling\n",
    "    data[\"Annual_Premium_Log\"] = np.log1p(data[\"Annual_Premium\"])\n",
    "    data[\"Vintage_Sqrt\"] = np.sqrt(data[\"Vintage\"])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    columns_to_scale = [\"Annual_Premium_Log\", \"Vintage_Sqrt\", \"Age\", \"Policy_Sales_Channel\"]\n",
    "    data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
    "\n",
    "    # Convert 'Vehicle_Age' into boolean features\n",
    "    vehicle_age_dummies = pd.get_dummies(data['Vehicle_Age'], prefix='Vehicle_Age')\n",
    "    data = pd.concat([data, vehicle_age_dummies], axis=1)\n",
    "\n",
    "    # Create age group categories and interaction terms\n",
    "    data[\"Age_Group\"] = pd.cut(data[\"Age\"], bins=[0, 30, 45, 60, 100], labels=[\"Young\", \"Adult\", \"Middle_Aged\", \"Senior\"])\n",
    "    age_group_dummies = pd.get_dummies(data[\"Age_Group\"], prefix=\"Age_Group\")\n",
    "    data = pd.concat([data, age_group_dummies], axis=1)\n",
    "\n",
    "    # Interaction features\n",
    "    for age_group in age_group_dummies.columns:\n",
    "        for vehicle_age in vehicle_age_dummies.columns:\n",
    "            data[f\"{age_group}_{vehicle_age}\"] = data[age_group] * data[vehicle_age]\n",
    "\n",
    "    # Additional features\n",
    "    data[\"Age_Squared\"] = data[\"Age\"] ** 2\n",
    "    data[\"Premium_to_Age_Ratio\"] = data[\"Annual_Premium_Log\"] / (data[\"Age\"] + 1e-5)\n",
    "    data[\"Retention_Likelihood\"] = (data[\"Vintage\"] > data[\"Vintage\"].median()).astype(int)\n",
    "    data[\"Age_VehicleDamage_Interaction\"] = data[\"Age\"] * data[\"Vehicle_Damage\"]\n",
    "\n",
    "    # Region-based features\n",
    "    data['Region'] = data['Policy_Sales_Channel'].apply(lambda x: 'North' if x <= 100 else 'South')\n",
    "    region_dummies = pd.get_dummies(data['Region'], prefix='Region')\n",
    "    data = pd.concat([data, region_dummies], axis=1)\n",
    "\n",
    "    # Handle categorical features\n",
    "    categorical_features = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Damage']\n",
    "    for feature in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        data[feature] = le.fit_transform(data[feature])\n",
    "\n",
    "    # Drop original categorical columns after encoding\n",
    "    data.drop(columns=[\"Age_Group\", \"Vehicle_Age\", \"Region\"], inplace=True)\n",
    "\n",
    "    # Separate ID column and response if training\n",
    "    id_column = data.pop(\"id\")\n",
    "    if is_train:\n",
    "        y = data.pop(\"Response\")\n",
    "        return data, y, id_column\n",
    "    return data, id_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column, n_std=3):\n",
    "    \"\"\"Remove outliers based on standard deviation.\"\"\"\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    return df[(df[column] >= mean - n_std * std) & (df[column] <= mean + n_std * std)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(X, y, n_features=30):\n",
    "    \"\"\"Select top n important features based on mutual information.\"\"\"\n",
    "    mi_scores = mutual_info_classif(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "    return mi_scores.nlargest(n_features).index.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization with Optuna\n",
    "\n",
    "In this notebook, we demonstrate the use of **Optuna** for hyperparameter optimization of machine learning models, specifically **XGBoost** and **LightGBM** classifiers. Hyperparameter tuning is a crucial step in improving model performance, as it helps to find the best combination of hyperparameters for a given dataset.\n",
    "\n",
    "#### Key Functions:\n",
    "\n",
    "1. **`optimize_xgb`**:\n",
    "    - This function uses Optuna to optimize hyperparameters for the **XGBoost** classifier.\n",
    "    - The hyperparameters being tuned include `n_estimators`, `learning_rate`, `max_depth`, `subsample`, `colsample_bytree`, `min_child_weight`, `gamma`, and `scale_pos_weight`.\n",
    "    - A cross-validation approach is used to evaluate the performance of the model with the suggested hyperparameters.\n",
    "\n",
    "2. **`optimize_lgbm`**:\n",
    "    - Similar to the `optimize_xgb` function, this function optimizes hyperparameters for the **LightGBM** classifier using Optuna.\n",
    "    - Hyperparameters tuned include `n_estimators`, `learning_rate`, `num_leaves`, `subsample`, `colsample_bytree`, `min_child_samples`, `reg_alpha`, and `reg_lambda`.\n",
    "    - The function also incorporates cross-validation to evaluate the performance.\n",
    "\n",
    "3. **`cross_validate`**:\n",
    "    - A utility function that performs **Stratified K-Fold cross-validation** to evaluate the model's performance.\n",
    "    - It splits the dataset into training and validation sets and computes the **ROC AUC score** for each fold.\n",
    "    - The average ROC AUC score is returned as the final evaluation metric.\n",
    "\n",
    "These functions are crucial for automating the process of hyperparameter tuning and ensuring that the models perform optimally on the given dataset.\n",
    "\n",
    "By leveraging Optuna’s optimization capabilities, we can efficiently search the hyperparameter space for both XGBoost and LightGBM models to achieve better predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgb(trial):\n",
    "    \"\"\"Optimize XGBoost hyperparameters using Optuna.\"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 1.0),\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    return cross_validate(model, X_train, y_train)\n",
    "\n",
    "def optimize_lgbm(trial):\n",
    "    \"\"\"Optimize LightGBM hyperparameters using Optuna.\"\"\"\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 100),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
    "        'is_unbalance': True,\n",
    "    }\n",
    "    model = LGBMClassifier(**params)\n",
    "    return cross_validate(model, X_train, y_train)\n",
    "\n",
    "\n",
    "def cross_validate(model, X, y, n_splits=5):\n",
    "    \"\"\"Perform cross-validation.\"\"\"\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred = model.predict_proba(X_val_cv)[:, 1]\n",
    "        scores.append(roc_auc_score(y_val_cv, y_pred))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    train_data = pd.read_csv(\"train.csv\")\n",
    "    test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    # Remove outliers in 'Annual_Premium' column\n",
    "    train_data = remove_outliers(train_data, \"Annual_Premium\")\n",
    "\n",
    "    # Preprocess data\n",
    "    X, y, _ = preprocess_data(train_data)\n",
    "    X_test, test_ids = preprocess_data(test_data, is_train=False)\n",
    "    \n",
    "    # Select top important features\n",
    "    top_features = select_important_features(X, y, n_features=30)\n",
    "    X = X[top_features]\n",
    "    X_test = X_test[top_features]\n",
    "\n",
    "    # Train-validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "    # Run Optuna studies\n",
    "    n_trials = 10\n",
    "    timeout = 600 \n",
    "\n",
    "    print(\"Optimizing XGBoost...\")\n",
    "    study_xgb = optuna.create_study(direction=\"maximize\")\n",
    "    study_xgb.optimize(optimize_xgb, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "    print(\"Optimizing LightGBM...\")\n",
    "    study_lgbm = optuna.create_study(direction=\"maximize\")\n",
    "    study_lgbm.optimize(optimize_lgbm, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "\n",
    "\n",
    "    # Train final models\n",
    "    best_xgb = XGBClassifier(**study_xgb.best_params).fit(X, y)\n",
    "    best_lgbm = LGBMClassifier(**study_lgbm.best_params).fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = {\n",
    "        \"XGBoost\": best_xgb.predict_proba(X_test)[:, 1],\n",
    "        \"LightGBM\": best_lgbm.predict_proba(X_test)[:, 1],\n",
    "    }\n",
    "    \n",
    "    # Ensemble predictions\n",
    "    ensemble_predictions = np.mean([predictions[\"XGBoost\"], predictions[\"LightGBM\"]], axis=0)\n",
    "    \n",
    "    # Save outputs\n",
    "    for model_name, preds in predictions.items():\n",
    "        output = pd.DataFrame({\"id\": test_ids, \"Response\": preds})\n",
    "        output.to_csv(f\"output_{model_name.lower()}.csv\", index=False)\n",
    "        print(f\"{model_name} predictions saved to output_{model_name.lower()}.csv\")\n",
    "\n",
    "    # Save ensemble predictions\n",
    "    ensemble_output = pd.DataFrame({\"id\": test_ids, \"Response\": ensemble_predictions})\n",
    "    ensemble_output.to_csv(\"output_ensemble.csv\", index=False)\n",
    "    print(\"Ensemble predictions saved to output_ensemble.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "The following table summarizes the performance metrics for the models used in this project. \n",
    "\n",
    "| **Model**                          | **Kaggle Score** |\n",
    "|------------------------------------|------------------|\n",
    "| **XGBoost**                        | 0.89325          |\n",
    "| **LightGBM**                       | 0.89314          |\n",
    "| **Deep Learning (Neural Network)** | 0.89194          |\n",
    "| **Tabnet**                          | 0.88446          |\n",
    "| **Decision Tree using Entropy V2**  | 0.86291          |\n",
    "| **Logistic Regression**            | 0.85110          |\n",
    "| **Decision Tree using Entropy V1**  | 0.82384          |\n",
    "| **Random Forest**                  | 0.82597          |\n",
    "| **Decision Tree using Gini V1**    | 0.82260          |\n",
    "| **KNN**                            | 0.78019          |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieving a Kaggle Score of 0.89452\n",
    "\n",
    "Our team was able to achieve a Kaggle score of **0.89452**, which was the highest among the models we tested, with the next best model having a Kaggle score of **0.89325**. This improvement was achieved through an effective **model ensembling** strategy.\n",
    "\n",
    "### Ensembling Approach\n",
    "\n",
    "The key idea behind our approach was to combine the predictions from multiple models to leverage their strengths and create a more robust prediction. Here’s how we went about it:\n",
    "\n",
    "1. **Identifying the Best Models**: \n",
    "   - We identified two of our top-performing models based on their individual Kaggle scores: **Model 1** (with a score of 0.89325) and **Model 2** (with a slightly lower score).\n",
    "   \n",
    "2. **Ensembling High-Differentiation Outputs**:\n",
    "   - Instead of simply averaging the predictions of the two models, we focused on selecting predictions that had the highest difference (most divergent outputs) between the two models. This helps capture more unique information and reduces the risk of overfitting.\n",
    "   \n",
    "3. **Further Ensembling**:\n",
    "   - Once we had the ensemble predictions from the initial two models, we fed these into a second ensembling step with predictions from additional models to further refine the results. This iterative process helped us to maximize the combined predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Used for Ensembling\n",
    "\n",
    "Here is the code we used to combine the predictions from two models and create the final ensemble prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two prediction files\n",
    "file1 = \"output1.csv\"  \n",
    "file2 = \"output2.csv\" \n",
    "\n",
    "# Read the prediction files\n",
    "pred1 = pd.read_csv(file1)\n",
    "pred2 = pd.read_csv(file2)\n",
    "\n",
    "if not pred1['id'].equals(pred2['id']):\n",
    "    raise ValueError(\"The 'id' columns in the two files do not match!\")\n",
    "\n",
    "# Average the predictions (ensemble)\n",
    "ensemble_preds = (pred1['Response'] + pred2['Response']) / 2\n",
    "\n",
    "ensemble_output = pd.DataFrame({\n",
    "    \"id\": pred1['id'],\n",
    "    \"Response\": ensemble_preds\n",
    "})\n",
    "\n",
    "ensemble_output.to_csv(\"output_ensemble11.csv\", index=False)\n",
    "print(\"Ensemble predictions saved to output_ensemble.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
